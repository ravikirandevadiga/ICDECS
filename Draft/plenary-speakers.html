
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<!-- Mirrored from 2019.ieeedatascience.org/plenary-speakers.php by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 14 May 2019 05:33:02 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>DSW 2019</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
<meta name="description" content="Learnig Journys" />
<meta name="keywords" content="Foto Walls" />
<meta name="robots" content="INDEX,FOLLOW" />
<link rel="icon" href="logo_dsw_small.png" type="image/x-icon" />
<link rel="shortcut icon" href="logo_dsw_small.png" type="image/x-icon" />
<link rel="shortcut icon" href="logo_dsw_small.png" type="image/x-icon" />
<!--[if lt IE 7]>
<script type="text/javascript">
//<![CDATA[
var BLANK_URL = 'http://127.0.0.1/magento/fotowalls/js/blank.html';
var BLANK_IMG = 'http://127.0.0.1/magento/fotowalls/js/spacer.gif';
//]]>
</script>
<![endif]-->
<link rel="stylesheet" type="text/css" href="css/fonts/fonts.css" media="all" />
<link rel="stylesheet" type="text/css" href="css/styles.css" media="all" />
<link rel="stylesheet" type="text/css" href="css/itemgrid.css" media="all" />
<link rel="stylesheet" type="text/css" href="css/grid12.css" media="all" />
<link rel="stylesheet" type="text/css" href="css/accordion.css" media="all" />
<link rel="stylesheet" type="text/css" href="css/ultra-megamenu.css" media="all" />
<link rel="stylesheet" type="text/css" href="css/custom.css" media="all" />
<link rel="stylesheet" type="text/css" href="css/responsive.css" media="all" />
<script src="js/jquery-1.7.2.min.js" type="text/javascript"></script>
<script src="js/jquery-noconflict.js" type="text/javascript"></script>
<script src="js/plugins/jquery.easing.min.js" type="text/javascript"></script>
<script src="js/plugins/jquery.flexslider.min.js" type="text/javascript"></script>
<script src="js/plugins/jquery.accordion.min.js" type="text/javascript"></script>
<script src="js/plugins/jquery.tabs.min.js" type="text/javascript"></script>
<script src="../s3-us-west-2.amazonaws.com/ieeeshutpages/gdpr/settings.js"></script>
<link rel="stylesheet" type="text/css" href="../cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css" />
<script src="../cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
<script>
    window.addEventListener("load", function(){
        window.cookieconsent.initialise(json)
    });
</script>
</head><body>
<div id="root-wrapper">  <!-- root-wrapper start -->
    <div class="wrapper"> <!-- wrapper start -->

        <noscript>
	<p><strong>JavaScript seems to be disabled in your browser.</strong><br /> You must have JavaScript enabled in your browser to utilize the functionality of this website.</p>
</noscript>
        <div class="page banner-small"><!-- page start -->

            <div class="header-container" id="top"> <!-- header start -->
            	<div class="header-top header container clearer">
        <div class="grid-full">
                <div class="header-main v-grid-container">
                        <div class="logo-wrapper grid12-4">
                                <h1 class="logo"><strong>DSW 2019</strong><a title="DSW 2019" href="index-2.html"><img alt="DSW 2019" src="images/logo_dsw.png" width="465" class="logo-img" /></a></h1>
                        </div>
                        <div class="other-logos grid12-8 clearer">
                                <div class="ieee-logo grid12-5">
                                        <img alt="IEEE" src="images/ieee.png">
                                </div>
                                <div class="ieee-logo-process grid12-7">
                                        <img alt="IEEE Signal Processing Society" src="images/signal-process.png">
                                </div>
                                <div class="clearer"></div>
                                <div class="top-link">
                                        <ul class="links">
                                                <li class="first"><a title="Organizing Committee" href="organizaing-committee.html">ORGANIZING COMMITTEE</a></li>
                                                <li><a title="Plenary Speakers" href="plenary-speakers.html">PLENARY SPEAKERS</a></li>
                                                <li class="register-now last"><a title="Paper Submission" href="author-information.html">PAPER SUBMISSION</a></li>
                                        </ul>
                                </div>
                        </div>
                </div>
        </div>
</div>             </div> <!-- header end -->
            <div class="in-col1">
                <div class="sidebar-wrap container">
                    <div class="col-left sidebar grid12-4 in-sidebar">

                        <!--
	------------------
	left side menu 	
	-----------------
-->
<div class="left-menu nav-container">
    <div class="grid-full" id="mobnav">
        <a href="javascript:vaid()" id="mobnav-trigger" class="active">
            <span class="trigger-icon">
            <span class="line"></span>
            <span class="line"></span>
            <span class="line"></span>
            </span>
            <span>Menu</span>
        </a>
    </div>
    <div class="clearer"></div>
    <div id="menu-option" class="menu-contain block-content">
      <ul class="accordion accordion-style1 vertnav vertnav-side clearer">
        <li class="level0 nav-1 active first"> <a href="index-2.html"> <span>Home</span> </a> </li>
        <li class="level0  nav-2"> <a href="call-for-papers.html"> <span>Call for Papers</span> </a> </li>
          <li class="level0  nav-3"> <a href="author-information.html"> <span>Paper Submission</span> </a> </li>
          <li class="level0  nav-3"> <a href="journal-submission.html"> <span>SPS Journal Submission</span> </a> </li>
        <li class="level0  nav-7"> <a href="program.html"> <span>Program</span> </a> </li>
        <li class="level0  nav-9"> <a href="tutorials.html"> <span>Tutorials</span> </a> </li>
        <li class="level0 active current  nav-8"> <a href="plenary-speakers.html"> <span>Plenary Speakers</span> </a> </li>
        <li class="level0  nav-10"> <a href="special-sessions.html"> <span>Special Sessions</span> </a> </li>
        <li class="level0  nav-6"> <a href="registration.html"> <span>Registration</span> </a> </li>
        <li class="level0  nav-15"> <a href="organizaing-committee.html"> <span>Organizing Committee</span> </a> </li>
        <li class="level0  nav-4"> <a href="paper-competition.html"> <span>Paper Awards</span> </a> </li>
        <li class="level0  nav-11"> <a href="presentation-guidelines.html"> <span>Presentation Guidelines</span> </a> </li>
        <li class="level0  nav-5"> <a href="student-support.html"> <span>Student Travel Support</span> </a> </li>
        <li class="level0  nav-12"> <a href="venue-host-city.html"> <span>Venue & Host City</span> </a> </li>
        <li class="level0  nav-13"> <a href="accommodation.html"> <span>Accommodation</span> </a> </li>
        <li class="level0  nav-14"> <a href="travel-visa.html"> <span>Travel & Visa</span> </a> </li>
      </ul>
      <div class="clearer"></div>
    </div>
    <script type="text/javascript">
        jQuery( "#mobnav-trigger" ).click(function() {
          jQuery( "#menu-option" ).slideToggle( "slow", function() {
            // Animation complete.
          });
        });
    </script>
</div>

<!--                        <div class="twiter-box slide-block">-->
<!--                            --><!--                        </div>-->

                    </div>
                </div>
            </div>

            <style>
                .bottom-three {
                    margin-bottom: 1cm;
                }
            </style>

            <div class="preface in-col1"> <!-- slideshow start -->
                <div class="the-slideshow-wrapper clearer">
    <div class="the-slideshow gen-slider-arrows2 gen-slider-pager1 gen-slider-pager1-pos-bottom-right grid-full no-gutter  ">
        <ul class="slides">
            <li class="slide">
                <div class="the-slideshow-wrapper container">
                    <div class="col-left slide_content in-sidebar">
                        <h1 class="slide-top-text"><span>Minneapolis, MN</span></h1>
                        <h4 class="slide-middle-text"><span>June 2-5, 2019</span></h4>
                        <h6 class="slide-bottom-text"><span>The second </span>IEEE Data Science Workshop</h6>
                    </div>
                    <div class="clearer"></div>
                </div>
                <img src="images/home_slide_4_dsw_thinner.jpg">
            </li>
            <!--<li class="slide">
                <img src="images/slide-1.jpg">
            </li>
            <li class="slide">
                <img src="images/slide-1.jpg">
            </li>-->
        </ul>
    </div>
    <script type="text/javascript">
    //&lt;![CDATA[
    jQuery(function($){ 
    jQuery('.the-slideshow').flexslider({
        namespace:          "",
        animation:          'fade',
        easing:             'easeInOutCubic',
        useCSS:             false,
        animationLoop:      1,
        smoothHeight:       0,
        slideshowSpeed:     10000,
        animationSpeed:     600,
        pauseOnHover:       1           });
    });
    //]]&gt;
    
    jQuery(document).ready(function(){
        /* Blank Paging Number */
        jQuery(".control-nav.control-paging li a").text(" ");
        
        /* Wrap container in paging-control */
        jQuery( "ol.control-nav" ).wrap( "<div class='control-nav-wrap container'></div>" );
        
        var x=jQuery(".slides img").offset();
        var y= x.top;
        jQuery(".slide_content").offset({top:y})
        
        jQuery(window).resize(function(){
            var x=jQuery(".slides img").offset();
            var y= x.top;
            jQuery(".slide_content").offset({top:y})
        /*  jQuery(".slide_content").each(function(){
                jQuery(this).offset({top:y})
              });*/
        });
        
        if (jQuery(window).width() > 960 ){
            jQuery("#mobnav").hide();
            jQuery("#menu-option").show();
        }
    });
    </script>
</div>            </div> <!-- slideshow end -->

            <div class="main-container col2-left-layout"> <!-- content start -->
                <div class="main container show-bg">
                    <div class="col-main grid12-8 grid-col2-main in-col2">
                        <div class="std">
                            <div class="page-title">
                                <h1 class="title">Plenary Speakers</h1>
                            </div>
                            <div class="plentut-list grid-full">
                                <ul class="ptlist">
                                    <h2>Monday, June 3 </h2>
                                    <h2>Morning </h2>
                                    <li>
                                        <div class="plentut-img left">
                                            <img src="images/Bertozzi.jpg" height="150" width="120" />
                                        </div>
                                        <div class="org-detail left">
                                            <h2>Andrea L. Bertozzi </h2>
                                            <strong> Prof. of Mathematics and Mechanical and Aerospace Engineering</strong><br>
                                            <strong> Betsy Wood Knapp Chair for Innovation and Creativity</strong><br>
                                            <strong>University of California Los Angeles</strong><br>
                                            <strong>USA</strong>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>
                                    <li>
                                        <div class="org-detail left">
                                            <h3> The Mathematics of Crime </h3>
                                            <p>
                                                I will review research related to the modeling and analysis of crime patterns, most notably crime hotspots and self-excitation of crime,, in particular gang crimes. The models include residential burglaries using PDEs, stochastic self-exciting point process models for repeat victimization and gang retaliation, and deep learning for real time prediction. The point process work has led to a predictive policing software program that is in use in over 50 cities worldwide. We have also been analyzing videos from police body worn cameras using machine learning methods for semi-supervised learning.
                                            </p>
                                            <p class="bottom-three"> More info about the speaker <a href="http://www.math.ucla.edu/~bertozzi/">here</a>. </p>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>

                                    <h2>Afternoon </h2>

                                    <li>
                                        <div class="plentut-img left">
                                            <img src="images/nowak.jpg" height="150" width="120" />
                                        </div>
                                        <div class="org-detail left">
                                            <h2>Robert D. Nowak </h2>
                                            <strong> Nosbusch Professor in Engineering</strong><br>
                                            <strong>University of Wisconsin-Madison</strong><br>
                                            <strong>USA</strong>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>
                                    <li>
                                        <div class="org-detail left">
                                            <h3> Active Machine Learning: From Theory to Practice </h3>
                                            <p>
                                                Machine learning has advanced considerably in recent years, but mostly in well-defined domains using huge amounts of human-labeled training data. Machines can recognize objects in images and translate text, but they must be trained with more images and text than a person can see in nearly a lifetime. Generating the necessary training data sets can require an enormous human effort. Active machine learning tackles this issue by designing learning algorithms that automatically and adaptively select the most informative data for labeling so that human time is not wasted on irrelevant or trivial examples. This lecture will cover theory, methods, and applications of active machine learning.
                                            </p>
                                            <p class="bottom-three"> More info about the speaker <a href="https://nowak.ece.wisc.edu/index.html">here</a>. </p>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>

                                    <h2>Tuesday, June 4 </h2>
                                    <h2>Morning </h2>
                                    <li>
                                        <div class="plentut-img left">
                                            <img src="images/yu.jpg" height="150" width="120" />
                                        </div>
                                        <div class="org-detail left">
                                            <h2>Bin Yu </h2>
                                            <strong> Chancellor's Professor</strong><br>
                                            <strong> Statistics, Electrical Engineering and Computer Science</strong><br>
                                            <strong>University of California, Berkeley</strong><br>
                                            <strong>USA</strong>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>
                                    <li>
                                        <div class="org-detail left">
                                            <h3> PCS Workflow, Interpretable Machine Learning, and DeepTune </h3>
                                            <p>
                                                In this talk, I'd like to discuss the intertwining importance and connections
                                                of three principles of data science: predictability, computability
                                                and stability (PCS) and the PCS workflow that is built on the three principles.
                                                I will also define interpretable machine learning (iML)
                                                through the PDR desiderata (Predictive accuracy, Descriptive accuracy and
                                                Relevancy) and discuss stability as a minimum requirement for interpretability.
                                                The principles and iML desiderata, PCS and PDR, will be
                                                demonstrated in the context of a collaborative project in neuroscience,
                                                DeepTune, for interpretable data results and testable hypothesis generation.
                                                If time allows, I will present proposed PCS inference that includes
                                                perturbation intervals and PCS hypothesis testing. PCS inference
                                                uses prediction screening and takes into account both data and model
                                                perturbations.  Last but not least, a PCS documentation is proposed
                                                based on Rmarkdown, iPython, or Jupyter Notebook, with publicly available,
                                                reproducible codes and narratives to back up human choices made throughout
                                                an analysis. (The PCS workflow and documentation are demonstrated in a
                                                genomics case study available on Zenodo.)
                                            </p>

                                            <p class="bottom-three"> More info about the speaker <a href="https://www.stat.berkeley.edu/~binyu/Site/Welcome.html">here</a>. </p>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>
                                    <h2>Afternoon </h2>
                                    <li>
                                        <div class="plentut-img left">
                                            <img src="images/RongJin.jpg" height="150" width="120" />
                                        </div>
                                        <div class="org-detail left">
                                            <h2>Rong Jin </h2>
                                            <strong>Principal Engineer</strong><br>
                                            <strong>Alibaba Group</strong><br>
                                            <strong>China</strong>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>
                                    <li>
                                        <div class="org-detail left">
                                            <h3> Optimization in Alibaba: Beyond Convexity</h3>
                                            <p>
                                                In this talk, we will talk about the recent developments in large-scale optimization that are beyond the conventional wisdom of convex optimization. I will specifically address three challenging problems that have found applications in many Alibaba businesses. In the first application, we study the problem of optimizing truncated loss functions that are of particularly importance when coming to learning from heavily tailed distributions. We show that, despite of its non-convexity, under appropriate condition, a variant of gradient descent could efficiently find the global optimal. In the second application, we study the problem of how to find local optimal in the case of non-convex optimization. We show that with introduction of appropriate random perturbation, we could find the local optimal at the rate of O(1/gamma^3) where gamma defines the suboptimality, which significantly improves the results of the existing studies. In the last application, we consider optimizing a continuous function over a discrete space comprised of a huge number of data points. The special instances of this problem include approximate nearest neighbor search and learning a quantized neural network. The most intriguing result from our study is that this optimization problem becomes relatively easier when the size of discrete space is sufficiently large. We provide results of both theoretical analysis and empirical studies.                                            </p>
                                            <p class="bottom-three"> More info about the speaker <a href="https://scholar.google.es/citations?user=CS5uNscAAAAJ&amp;hl=en&amp;oi=ao">here</a>. </p>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>

                                    <h2>Wednesday, June 5 </h2>
                                    <h2>Morning </h2>
                                    <li>
                                        <div class="plentut-img left">
                                            <img src="images/Xiao-Li-Meng.jpg" height="150" width="120" />
                                        </div>
                                        <div class="org-detail left">
                                            <h2>Xiao-Li Meng </h2>
                                            <strong>Whipple V. N. Jones Professor of Statistics</strong><br>
                                            <strong>Harvard University</strong><br>
                                            <strong>USA</strong>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>
                                    <li>
                                        <div class="org-detail left">
                                            <h3> Is it a Computing Algorithm or a Statistical Procedure: Can you tell or should you care?</h3>
                                            <p>
                                                The line between computing algorithms and statistical procedures is becoming increasingly blurred, as practitioners are now typically given a black box, which turns data into an “answer”. Is such a black box a computing algorithm or a statistical procedure? Does it matter that we know which is which? This talk reports my contemplations of such questions that originated in my taking part in a project that investigates the self-consistency principle introduced by Efron (1967). We will start with a simple regression problem to illustrate a self-consistency method and the audiences will be invited to contemplate whether it is a magical computing algorithm or a powerful statistical procedure. We will then discuss how such contemplations have played critical roles in developing the self-consistency principle into a “Likelihood-Free EM algorithm” for semi/non-parametric estimation with incomplete data and under an arbitrary loss function, capable of addressing wavelets de-noising with irregularly spaced data as well as variable selection via LASSO-type of methods with incomplete data. Throughout the talk, the audience will also be invited to consider a widely open problem: how to formulate in general the trade-off between statistical efficiency and computational efficiency? (This talk is based on joint work with Thomas Lee and Zhan Li.)
                                            </p>
                                            <p class="bottom-three"> More info about the speaker <a href="https://statistics.fas.harvard.edu/people/xiao-li-meng">here</a>. </p>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>
                                    <h2>Afternoon </h2>
                                    <h5>(Shared with the <a href="http://www.gspworkshop.org/plenary-speakers.php">Graph Signal Processing workshop</a>)</h5>
                                    <li>
                                        <div class="plentut-img left">
                                            <img src="images/neville.jpg" height="150" width="120" />
                                        </div>
                                        <div class="org-detail left">
                                            <h2>Jennifer Neville</h2>
                                            <strong> Prof. of  Computer Science and Statistics</strong><br>
                                            <strong>Purdue University</strong><br>
                                            <strong>USA</strong>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>
                                    <li>
                                        <div class="org-detail left">
                                            <h3> Towards Relational AI &ndash; The good, the bad, and the ugly of learning over networks </h3>
                                            <p>
                                            In the last 20 years, there has been a great deal of research on machine learning methods for graphs, networks, and other types of relational data. By moving beyond the independence assumptions of more traditional ML methods, relational models are now able to successfully exploit the additional information that is often observed in relationships among entities. Specifically, network models are able to use relational information to improve predictions about user interests, behavior, and interactions, particularly when individual data is sparse. The tradeoff however, is that the heterogeneity, partial-observability, and interdependence of large-scale network data can make it difficult to develop efficient and unbiased methods, due to several algorithmic and statistical challenges. In this talk, I will discuss these issues while surveying several general approaches used for relational learning in large-scale social and information networks. In addition, to reflect on the movement toward pervasive use of the models in personalized online systems, I will discuss potential implications for privacy, polarization of communities, and spread of misinformation.
                                            </p>

<!--                                            <p>-->
<!--                                            <strong>Biography</strong><br>-->
<!--                                            Jennifer Neville is the Miller Family Chair Associate Professor of Computer Science and Statistics at Purdue University. She received her PhD from the University of Massachusetts Amherst in 2006. She is currently PC chair of the 19th SIAM International Conference on Data Mining. She was an elected member of the AAAI Executive Council from 2015-2018. In 2016 she was PC chair of the 9th ACM International Conference on Web Search and Data. In 2012, she was awarded an NSF Career Award, in 2008 she was chosen by IEEE as one of "AI's 10 to watch", and in 2007 was selected as a member of the DARPA Computer Science Study Group. Her work, which includes 100+ peer-reviewed publications with more than 7500 citations, focuses on developing machine learning and AI methods for complex relational domains, including social, information, and physical networks.-->
<!--                                            </p>-->
                                            <p class="bottom-three"> More info about the speaker <a href="https://www.cs.purdue.edu/homes/neville/">here</a>. </p>
                                        </div>
                                        <div class="clearer"></div>
                                    </li>


<!--                                    <p class="bottom-three"> Additional plenary speakers will be confirmed shortly.</p>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <strong>Biography</strong><br>-->
<!--                                        <p>Yuri I. Abramovich received the Dipl. Eng (Hons.) degree (1967) and the Cand. Sci degree (Ph.D. equivalent, 1971), both from Odessa Polytechnic University, Ukraine and got the D.Sc. degree in radar from the Leningrad Institute for Avionics in 1981. From 1968-1994, he was with Odessa Polytechnic University.  In 1980, he got the Award from Scientific Council for Radio physics of the Soviet Academy of Science for his contribution to adaptive processing in various defense radars. His diagonally loaded sample matrix inversion technique, published in 1980, became the commonly used in adaptive beamforming. From 1994 -2010, he was with the Cooperative Research Centre and the Australian Defense Science and Technology Organization in Adelaide. During this period, Dr. Abramovich developed the new concept of surface-wave radar (the US Patent 20030142011). The new methodology of OTHR array calibration was awarded by the Oliver Lodge Premium of IEE in 2001. Dr. Abramovich became a co-author of the novel OTH radar MIMO technology that demonstrated significant radar performance enhancement. In 2010 Dr. Y. Abramovich received the Australian Chief Scientist Award for “exceptional contribution to radar signal processing.” In 2011, Dr. Abramovich joined the W R System , Fairfax, VA ,working on the US OTHR.</p>-->
<!--                                        <p>In 2008, he was elevated to the IEEE Fellow grade for “contributions to adaptive signal processing for detection and estimation in radar arrays.” In 2011, he was elected to the IEEE AESS Board of Governors.  In 2011, he received the Technical Achievement Award for “fundamental contribution to adaptive array processing for radar” from the European Association for Signal Processing.  In 2014 he was awarded by the IEEE Dennis J. Pickard Medal for Radar Technologies and Applications “For seminal contributions to adaptive radar signal processing algorithms and Over-the-Horizon Radar.”</p>-->
<!--                                        <p class="divider">Dr. Abramovich is an author of 110 journal articles, 140 Western conference papers and chapters in six books.</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                </ul>-->
<!--                            </div>-->
<!--                            <div class="plentut-list grid-full">-->
<!--                                <ul class="ptlist">-->
<!--                                    <li>-->
<!--                                        <div class="plentut-img left">-->
<!--                                            <img src="images/baraniuk.jpg" height="134" width="99" />-->
<!--                                        </div>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <h2>Richard G. Baraniuk</h2>-->
<!--                                        <strong>Victor E. Cameron Professor</strong><br>-->
<!--                                        <strong>Rice University,</strong><br>-->
<!--                                        <strong>USA</strong>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <h3>A Probabilistic Theory of Deep Learning</h3>-->
<!--                                        <p>A grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. For instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks that routinely yield pattern recognition systems with near- or super-human capabilities. But a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on the Deep Rendering Model: a generative probabilistic model that explicitly captures latent nuisance variation. By relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks and random decision forests, providing insights into their successes and shortcomings, a principled route to their improvement, and new avenues for exploration.</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <strong>Biography</strong><br>-->
<!--                                        <p class="divider">Richard G. Baraniuk is the Victor E. Cameron Professor of Electrical and Computer Engineering at Rice University.  His research interests lie in new theory, algorithms, and hardware for sensing, signal processing, and machine learning. He is a Fellow of the IEEE and AAAS and has received national young investigator awards from the US NSF and ONR, the Rosenbaum Fellowship from the Isaac Newton Institute of Cambridge University, the ECE Young Alumni Achievement Award from the University of Illinois, the Wavelet Pioneer and Compressive Sampling Pioneer Awards from SPIE, and the IEEE Signal Processing Society Technical Achievement Award.  His work on the Rice single-pixel compressive camera has been widely reported in the popular press and was selected by MIT Technology Review as a TR10 Top 10 Emerging Technology.  For his teaching and education projects, including Connexions (cnx.org) and OpenStax College (openstaxcollege.org), he has received the C. Holmes MacDonald National Outstanding Teaching Award from Eta Kappa Nu, the Tech Museum of Innovation Laureate Award, the Internet Pioneer Award from the Berkman Center for Internet and Society at Harvard Law School, the World Technology Award for Education, the IEEE-SPS Education Award, the WISE Education Award, and the IEEE James H. Mulligan, Jr. Medal for Education.</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                </ul>-->
<!--                            </div>-->
<!--                            <div class="plentut-list grid-full">-->
<!--                                <ul class="ptlist">-->
<!--                                    <li>-->
<!--                                        <div class="plentut-img left">-->
<!--                                            <img src="images/flandrin.jpg" height="134" width="99" />-->
<!--                                        </div>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <h2>Patrick Flandrin</h2>-->
<!--                                        <strong>Department of Physics</strong><br>-->
<!--                                        <strong>CNRS and École Normale Supérieure de Lyon,</strong><br>-->
<!--                                        <strong>France</strong>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <h3>Graphs as Signals</h3>-->
<!--                                        <p>Graphs are ubiquitous for representing interactions in networks, be they physical, biological or social. Whereas numerous studies are intended to develop methods for analyzing signals over graphs, it will here be shown how the analysis of graph structures themselves can be performed by using tools borrowed from signal processing. The core of the approach is to build a distance map from the adjacency matrix of a graph, from which a collection of signals can be obtained thanks to a multidimensional scaling technique. Spectral features of the so-obtained signals can then be derived, with distinctive features for graph structures of different natures (regular, Erdös-Rényi, communities, scale-free, etc.). Various issues related to this perspective will be discussed, including efficient ways of inverting the transformation on the basis of a few components only, thus paving the way for « graph filtering ». An extension to dynamic graphs will also be considered, in which the time evolution of spectral features defines a matrix that can be factorized non-negatively. (Based on joint work with R. Hamon, P. Borgnat and C. Robardet.)</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <strong>Biography</strong><br>-->
<!--                                        <p class="divider">Patrick Flandrin received the engineer degree from ICPI Lyon, France, in 1978, and the Doct.-Ing. and Docteur d’État degrees from INP Grenoble, France, in 1982 and 1987, respectively. He joined CNRS in 1982, where he is currently Research Director. Since 1991, he has been with the Signals, Systems and Physics Group, within the Physics Department at ENS de Lyon, France. He is currently President of GRETSI, the French Association for Signal and Image Processing. His research interests include mainly nonstationary signal processing (with emphasis on time-frequency and time-scale methods), scaling stochastic processes and complex systems. He published over 250 research papers and authored one monograph in those areas. Dr. Flandrin was awarded the Philip Morris Scientific Prize in Mathematics (1991), the SPIE Wavelet Pioneer Award (2001), the Prix Michel Monpetit from the French Academy of Sciences (2001) and the Silver Medal from CNRS (2010). Past Distinguished Lecturer of the IEEE Signal Processing Society (2010-2011), he is a Fellow of the IEEE (2002) and of EURASIP (2009), and he has been elected member of the French Academy of sciences in 2010.</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                </ul>-->
<!--                            </div>-->
<!--                            <div class="plentut-list grid-full">-->
<!--                                <ul class="ptlist">-->
<!--                                    <li>-->
<!--                                        <div class="plentut-img left">-->
<!--                                            <img src="images/moura.jpg" height="134" width="99" />-->
<!--                                        </div>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <h2>José M. F. Moura</h2>-->
<!--                                        <strong>Philip L. and Marsha Dowd University Professor</strong><br>-->
<!--                                        <strong>Carnegie Mellon University,</strong><br>-->
<!--                                        <strong>USA</strong>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <h3>Network Processes</h3>-->
<!--                                        <p>Traditionally, in engineering, dynamic systems are lumped systems described by an ordinary or partial differential or difference equation. In many recent applications of interest, for example, in large scale networked infrastructures, in social networks, in populations, systems are networks of possibly simple components or agents, and the system (network) state evolves through local interactions among its components. We explore methods to study the dynamics of these network processes and how to derive the system global behaviors that arise from the local interactions among the system components. (Work with June Zhang.)</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <strong>Biography</strong><br>-->
<!--                                        <p class="divider">José M. F. Moura is the Philip L. and Marsha Dowd University Professor at CMU, with interests in data science. He is 2016 IEEE VP for Technical Activities, was IEEE Board Director, President of the IEEE Signal Processing Society (SPS), and Editor in Chief for the Transactions on SP. Moura received the IEEE SPS Technical Achievement Award and Society Award. He is Fellow of the IEEE and of AAAS, corresponding member of the Academy of Sciences of Portugal, Fellow of the US National Academy of Innovation, and member of the US National Academy of Engineering.</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                </ul>-->
<!--                            </div>-->
<!--                            <div class="plentut-list grid-full">-->
<!--                                <ul class="ptlist">-->
<!--                                    <li>-->
<!--                                        <div class="plentut-img left">-->
<!--                                            <img src="images/varshney.jpg" height="134" width="99" />-->
<!--                                        </div>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <h2>Pramod K. Varshney</h2>-->
<!--                                        <strong>Distinguished Professor</strong><br>-->
<!--                                        <strong>Syracuse University,</strong><br>-->
<!--                                        <strong>USA</strong>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <h3>Distributed Inference in the Presence of Byzantines</h3>-->
<!--                                        <p>In this talk, we discuss the problem of Byzantines in the context of Distributed Inference Networks. Distributed inference networks have many applications including military surveillance, cognitive radio networks and smart grid. A distributed inference network typically consists of local sensors sending information to a central processing unit (known as the Fusion Center) that is responsible for inference.  The network may contain malicious sensors that may engage in data falsification which can result in a wrong inference at the Fusion Center. Drawing parallel to the "Byzantine Generals Problem", the local sensors are the generals who try to make a decision in the presence of traitors called "Byzantines". We present an overview of recent research on this problem. Discussion includes the susceptibility of distributed inference networks to Byzantines, and then the possible protection of these networks through mitigation of Byzantines. A game theoretic formulation of the problem is also discussed. Several applications are considered and some avenues for further research are provided.</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                    <li>-->
<!--                                        <div class="org-detail left">-->
<!--                                        <strong>Biography</strong><br>-->
<!--                                        <p>Pramod K. Varshney received the B.S. degree in electrical engineering and computer science (with highest honors), and the M.S. and Ph.D. degrees in electrical engineering from the University of Illinois at Urbana-Champaign in 1972, 1974, and 1976 respectively. Since 1976 he has been with Syracuse University, Syracuse, NY where he is currently a Distinguished Professor of Electrical Engineering and Computer Science and the Director of CASE: Center for Advanced Systems and Engineering.  He is also an Adjunct Professor of Radiology at Upstate Medical University in Syracuse, NY. His current research interests are in distributed sensor networks and data fusion, detection and estimation theory, wireless communications, and security. He has published extensively.</p>-->
<!--                                        <p>While at the University of Illinois, Dr. Varshney was a James Scholar, a Bronze Tablet Senior, and a Fellow.  He is a member of Tau Beta Pi and is the recipient of the 1981 ASEE Dow Outstanding Young Faculty Award.  He was elected to the grade of Fellow of the IEEE in 1997 for his contributions in the area of distributed detection and data fusion. In 2000, he received the Third Millennium Medal from the IEEE and Chancellor's Citation for exceptional academic achievement at Syracuse University. He is the recipient of the IEEE 2012 Judith A. Resnik Award and an honorary doctorate from Drexel University in June 2014. He recently served as a distinguished lecturer for the AES society of the IEEE. He is on the editorial boards of Journal on Advances in Information Fusion and IEEE Signal Processing Magazine. He was the President of International Society of Information Fusion during 2001.</p>-->
<!--                                        </div>-->
<!--                                        <div class="clearer"></div>-->
<!--                                    </li>-->
<!--                                </ul>-->
<!--                            </div>-->
<!--                            <div class="clearer"></div>-->
                        </div>

                        <!-- Mobile Tiwter -->
<!--                        <div class="mb-twiter-box slide-block">-->
<!--                            --><!--                        </div>-->

                    </div>

                    <div class="col-left sidebar grid12-4 grid-col2-sidebar empty-sidebar">
    &nbsp;
</div>
                </div>
            </div> <!-- content end -->

            <div class="footer-container"> <!-- footer start -->
                <div class="footer-primary-container section-container">
                    <div class="footer-primary footer container">
                        <div class="grid-full footer-links">
                            <p><strong>DSW - </strong>IEEE Data Science Workshop</p>
                            <p><a href="https://www.ieee.org/security-privacy.html"><strong>IEEE Privacy Policy </strong></a> | <a href="https://www.ieee.org/conferences/event-terms-and-conditions.html"><strong>IEEE Event Terms and Conditions </strong></a></p>
<!--                            <p><a href="#"><strong>Contact</strong></a> | <a href="https://www.ieee.org/about/corporate/governance/p9-26.html"><strong>IEEE Nondiscrimination Policy </strong></a> | <strong>Copyright IEEE Signal Processing Society</strong> - All Rights Reserved.</p>-->
                            <p><a href="https://www.ieee.org/about/corporate/governance/p9-26.html"><strong>IEEE Nondiscrimination Policy </strong></a> | <strong>Copyright IEEE Signal Processing Society</strong> - All Rights Reserved.</p>
                        </div>
                    </div>
                </div>            </div> <!-- footer end -->

        </div> <!-- page end -->
    </div> <!-- wrapper end -->
</div> <!-- root-wrapper end -->
</body>
